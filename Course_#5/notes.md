# Syntax and formal grammars
_Benoît Sagot_
_19/02/2018_

## Syntax
Branch of linguistics that studies the structural properties of sentences.
How words organized themself into sentences? It has no direct connection with meaning.

### Syntax and NLP
Crossroads between:
+ _syntax per se_: introspection, corpus studies (take a look with different statistics on it to extract pattern), psycholinguistics / neurolinguistics (ask questions to people and establish survey to different stimuli / try to understand what is happening in the brain while processing language)
+ formal grammars: what formal devices do we need to represent such structures?
+ language resources: treebanks (collection of sentences for which syntactic structures has been manualy added) ; syntactic lexicons (ressource that contains a lot of ressources about the syntactic properties of wordforms)

## Parsing
To build the syntactic structure of sentence is called parsing.
It's a key step in NLP systems. Parsing will be the topic of the next class and today's assignment

# Syntactic structures

## Structures in trees

### A first version Constituent structure
Underlying idea: sequnces of words belonging together form **constituents** in a **hierarchical way**. Forming trees

### How do we label the nodes?
_((the) boy) likes ((a) girl)_
build constituents so each one has exactly one non-bracketed word, called its head.
((the) boy): noun phrase, _boy_ being the head of _the_, and beeing a noun.
**PoS tagging**: clustering task

### Constituency trees
some limitations
More useful to insert PoS as immediate ancestor of leaf node
**Lexical anchors**: special connection of _the boy likes a girl_ to the trees.

### Dependencies
Underlying idea: each word is governe by another word, except for the main word of the sentence. A link between a word and its governor is a dependency. Such links can be labelled with dependency types.
This is a dependency tree.

## Dependencies Vs constituents
Dependency to constituency: mising label for internal nodes
Constituency to dependency: mising the head information for each constituent.

### Constituency trees: specifying heads
+ With two types of edges.
+ Head percolation table: specifying rules (NP (DET N*): noun will be the head of this constituent ; S(... V\* ...) V will be the head of the constituent)

## Key observation
Dependency structures look like semantic structures
Constituency structures are still very important, especially for configurational languages such as English where word orders matters (**fixed structures**)

## Non-projective dependencies
_une fille est entrée qui portant un chapeau_
lot of crossing edges in constituency tree

## Control, raising and attribution
_Pierre veut dormir_
Non-tree case: _Pierre_ is sub of _veut_ which as _dormir_ as object. But _dormir_ has _Pierre_ as deep-subject.

## Overall objective
Find a formal device to build constituency tree and semantic like structure. We'll limit ourselves to projectivem tree-like tree

# Formal grammars

## Language
Language is a set of words over an alphabet $T$, called the vocabulary. A language is a subset of $T$, $T^*$
Examples:
+ $L_1 = \{a, b\}$
+ $L_2 = \{a, b, \epsilon\}$
+ $L_3 = \{a^n b^n \mid n \in \mathbb{N}\}$ (aabb, ab, {}, aaaaabbbbb)
+ $L_4 = \{ww^{-1} \mid w \in T\}$

## Grammar
$$
G = (V, T, S, P)
$$

where:
+ $V$ is a finite set of objects called variables
+ $T$ is a finite set of objects called terminal symbols
+ $S \in V$ is a special symbol called the start variable
+ $P$ is a finite set of productions, or rewritting rules

$\epsilon$ is the empy string

## Grammar example
$$
G = (\{S\}, \{a,b\}, S, P)
$$

With the set of rules $P$:
$$
S \rightarrow aSb
$$

$$
S \rightarrow \epsilon
$$
aabb is a sentence in the language generated by Gm whle aaSbb is a sentential form

 Does this sentence belong in the language or not is the recognition problem

# Context-free languages

## Context-free grammars CFGs
lower cases for terminal words.

## Derivation tree
Sequences of rules applied / history of the derivation
_Pierre mange souvent des pommes de terre_

Some problems:
+ _pommes de terre_ is one sementical word, and should be one node
+ _des_ is on top of _pommes de terre_
+ node w/o anchors

# Tree Substitution Grammars

## Context-free grammars
We replace rewriting rules by equivalent **elementary trees**, in order to plug rules in a _lego-like_ way.
With the elementary trees, we are not stucked with 2 level nodes, and then solve the compound word problem. (one problem solved with _des pommes de terre_)

# Lexicalisation

## Lexicalisation
We had non anchored nodes, and nodes anchored with several nodes.
Each elemenraty tree should have at least one anchor.
Every elementray tree in the grammar which have more than one anchor should contain a unique sementic word

## Lexicalised tree subsitution grammar
We have plugged _souvent_ in _mange_ but we have one semantic word in each elementary tree. But _des_ is still on top of _pommes de terre_.

# Tree adjunction grammars

## Arguments and modifiers
**arguments** and **modifiers**:
+ _eat_ implies that a person eats and somewhat is eaten
+ a modfier is self-standing, independant, and add an information. _this morning_ add informations not required by _eat_

Direct object are always arguments, while circumstancial complements are modifiers.

Tree adjoining grammars (TAGs)

## The adjunction operation
New elementary tree with a start, called auxiliary (elementary) tree. _souvent_ is a modifier for _mange_.
It has a root ($V$) and a footnote with the same symbol ($V^*$) (the path between $V$ and $V^*$ is called spine) + a terminal symbol (adv)
The adjunction operation splits the $V$ and replace it with the spine. It leaves a trace ($V \rightarrow V$) (we adjunct the elementary tree a in elementary tree 1)

In derivation tree we must now represent 2 distinct operations:
+ substitution represented by a full
+ an adjunction represented with a dashed line

We will do the same for _des pommes de terre_: _des_ is a modifier for a noun, with the spine on the right side.

## Wrapping
We can have things on both side of the spine. The adjunction operation with the wrapping tree is possible.

_Online question 1: How do (non necessarily lexicalised) CFGs and TAGs compare in terms of expressive power? Let us consider the following languages: $\{a^nb^n\}$, $\{a^nb^nc^n\}$, $\{a^nb^nc^nd^n\}$


# Complexity intermezzo

## Parsing complexity

n is the size of the input sentence.
Adding the adjunction operation adds extra expressive power but add extra complexity in parsing.

# Tree insertion grammars
Adjunction without wrapping adjunctions

## Getting rid of wrapping adjunctions
Obtaining tree insertions grammars (TIG)

## Converting a TIG into a CFG
$$
S \rightarrow NP \quad VP_{left} \quad v \quad NP \quad VP_{right}
$$

## TAGs and TIGs in practice
Very time consuming
Two major ways out:
+ use a more abstract way to represent grammar rules, that can generate TAG (or a TIG): this is how one of th best performing parsers for French was developped (FRMG)
+ extract it from a treebank

# Probabilistic CFGs

## PCFGs
Discrete extension of CFGs
Assign probability to each sentence in the language generated by the grammar.
All the possibilities to rewrite a symbol have to sum to one.

# A few words on shared parse trees

## Multiple analyses: a lot of redundancies
Because it's a CFG, it's just about plugging. We can just use a representation form that capture the redundancies.

## Multiple analyses: instanciation
Grammar like representation of the first tree

## Towards arse forests
The instanciated grammar defines a language constaining the instanciated iput string, and allows for two parses for this instanciated string. We can represent this grammar as an AND-OR graph

# The CYK algorithm
Simplest algorithm for context tree parsing on any context free grammar

## A membership problem
The basic Cocke-Younger-Kasami, a.k.a CYK algorithm
The membership problem is simple: given an CFG and an input string, answer the following question: _Is the input string in the language defined by the CFG?_
The CFG mustbe in Chomsky normal form.
**Binarise your grammar by adding intermediate node to obtain in Chomsky normal form**

## Basic idea
Iterative algorithm (dynamic programming)

_Online question 2: What is the complexity of the CYK algoritm with respect to $n$, the length of the input string? $O(n^3)$, $O(n^4)$, $O(n^5)$, Other_
_$O(n^3)$ is the answer, because we only need 3 indices to compute the algorithm_

# Practical assignment

Develop a probabilistic parser using the CYK algorithm + PCFG model
